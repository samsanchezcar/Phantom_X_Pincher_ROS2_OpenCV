<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&height=180&section=header&text=PhantomX%20Pincher%20X100%20%E2%80%A2%20ROS%202%20%E2%80%A2%20OpenCV&fontSize=28&desc=Proyecto%20Final%20de%20Rob%C3%B3tica%20%E2%80%A2%20Universidad%20Nacional%20de%20Colombia&descSize=14&animation=fadeIn" width="100%" />
</div>

---

# ğŸ¤– PhantomX Pincher X100 â€” ROS 2 Jazzy Â· OpenCV Â· Pick & Place

<div align="center">

![ROS 2](https://img.shields.io/badge/ROS%202-Jazzy-blue?style=for-the-badge&logo=ros)
![Python](https://img.shields.io/badge/Python-3.12-blue?style=for-the-badge&logo=python)
![OpenCV](https://img.shields.io/badge/OpenCV-4.x-green?style=for-the-badge&logo=opencv)
![PyQt5](https://img.shields.io/badge/PyQt5-GUI-green?style=for-the-badge&logo=qt)
![Ubuntu](https://img.shields.io/badge/Ubuntu-24.04-orange?style=for-the-badge&logo=ubuntu)
![MoveIt](https://img.shields.io/badge/MoveIt2-Motion%20Planning-red?style=for-the-badge)

**Sistema completo de manipulaciÃ³n robÃ³tica con detecciÃ³n de figuras geomÃ©tricas y arquitectura distribuida ROS 2**

[ğŸ“¹ Ver Demo](#-videos-demostrativos) â€¢ [ğŸš€ InstalaciÃ³n](#-instalaciÃ³n-y-configuraciÃ³n) â€¢ [ğŸ“– DocumentaciÃ³n](#-tabla-de-contenidos) â€¢ [ğŸ‘¥ Equipo](#-autores)

</div>

---

## ğŸ“– Resumen Ejecutivo

Sistema robÃ³tico avanzado de **Pick & Place** con visiÃ³n artificial para el robot **PhantomX Pincher X100**. Implementa una arquitectura distribuida de nodos ROS 2 donde:

- **`opencv_detector`** (Publisher Node) â€” Captura video, detecta figuras geomÃ©tricas y publica coordenadas 3D
- **`pincher_controller`** (Subscriber Node) â€” Controla el robot, ejecuta pick & place y proporciona interfaz grÃ¡fica

El sistema detecta y clasifica automÃ¡ticamente figuras geomÃ©tricas (cÃ­rculo, cuadrado, rectÃ¡ngulo, pentÃ¡gono) de color naranja sobre un disco blanco de referencia, calcula sus coordenadas polares y cartesianas, y ejecuta rutinas autÃ³nomas de recolecciÃ³n y clasificaciÃ³n.

### CaracterÃ­sticas Principales

- âœ… **Arquitectura Distribuida ROS 2** â€” Nodos independientes con comunicaciÃ³n por tÃ³picos
- âœ… **DetecciÃ³n AutomÃ¡tica** â€” 4 tipos de figuras geomÃ©tricas con scoring multicriterio
- âœ… **Coordenadas Polares** â€” CÃ¡lculo en tiempo real respecto al disco de referencia
- âœ… **CinemÃ¡tica Inversa** â€” Algoritmo Levenberg-Marquardt con mÃºltiples semillas
- âœ… **GUI Profesional** â€” Interfaz moderna con PyQt5 y 8 pestaÃ±as de control
- âœ… **VisualizaciÃ³n Avanzada** â€” IntegraciÃ³n con RViz2 y Robotics Toolbox
- âœ… **Hardware & Software** â€” Compatible con robot real y visualizaciÃ³n simulada

---

## ğŸ¥ Videos Demostrativos

<div align="center">

| Video | DescripciÃ³n | Enlace |
|:-----:|:------------|:------:|
| ğŸ¬ **Demo Final** | Sistema completo con OpenCV y Pick & Place | [![YouTube](https://img.shields.io/badge/YouTube-Ver%20Video-red?style=flat-square&logo=youtube)](https://youtu.be/jv4XF2xeA04) |
| ğŸ”§ **Demo Preliminar** | Funcionamiento bÃ¡sico sin visiÃ³n artificial | [![YouTube](https://img.shields.io/badge/YouTube-Ver%20Video-red?style=flat-square&logo=youtube)](https://youtu.be/obf1X0HfMZE) |

</div>

---

## ğŸ§¾ Autores

<div align="center">

| Autor | Correo | GitHub |
|:------|:-------|:-------|
| **Samuel David Sanchez Cardenas** | samsanchezca@unal.edu.co | [![GitHub](https://img.shields.io/badge/GitHub-%40samsanchezcar-181717?style=flat&logo=github)](https://github.com/samsanchezcar) |
| **Santiago Ãvila Corredor** | savilaco@unal.edu.co | [![GitHub](https://img.shields.io/badge/GitHub-%40Santiago--Avila-181717?style=flat&logo=github)](https://github.com/Santiago-Avila) |
| **Santiago MariÃ±o CortÃ©s** | smarinoc@unal.edu.co | [![GitHub](https://img.shields.io/badge/GitHub-%40mrbrightside8-181717?style=flat&logo=github)](https://github.com/mrbrightside8) |
| **Juan Ãngel Vargas RodrÃ­guez** | juvargasro@unal.edu.co | [![GitHub](https://img.shields.io/badge/GitHub-%40juvargasro-181717?style=flat&logo=github)](https://github.com/juvargasro) |
| **Juan JosÃ© Delgado Estrada** | judelgadoe@unal.edu.co | [![GitHub](https://img.shields.io/badge/GitHub-%40Juan--delgado1-181717?style=flat&logo=github)](https://github.com/Juan-delgado1) |

**Universidad Nacional de Colombia â€¢ Facultad de IngenierÃ­a MecatrÃ³nica**  
**RobÃ³tica â€¢ Semestre 2025-1**

</div>

---

## ğŸ“‹ Tabla de Contenidos

1. [IntroducciÃ³n](#-introducciÃ³n)
2. [Objetivos](#-objetivos)
3. [Arquitectura del Sistema](#-arquitectura-del-sistema)
   - [Diagrama de Nodos ROS 2](#diagrama-de-nodos-ros-2)
   - [Flujo de Datos](#flujo-de-datos)
4. [InstalaciÃ³n y ConfiguraciÃ³n](#-instalaciÃ³n-y-configuraciÃ³n)
5. [Estructura de Paquetes](#-estructura-de-paquetes)
6. [Sistema de VisiÃ³n OpenCV](#-sistema-de-visiÃ³n-opencv)
   - [Nodo opencv_detector](#nodo-opencv_detector)
   - [Pipeline de DetecciÃ³n](#pipeline-de-detecciÃ³n)
   - [CÃ¡lculo de Coordenadas](#cÃ¡lculo-de-coordenadas)
7. [Controlador Principal](#-controlador-principal)
   - [Nodo pincher_controller](#nodo-pincher_controller)
   - [CinemÃ¡tica Inversa](#cinemÃ¡tica-inversa)
   - [Rutina Pick & Place](#rutina-pick--place)
8. [Interfaz GrÃ¡fica (GUI)](#-interfaz-grÃ¡fica-gui)
9. [Nodos y TÃ³picos ROS 2](#-nodos-y-tÃ³picos-ros-2)
10. [ConfiguraciÃ³n de CÃ¡mara](#-configuraciÃ³n-de-cÃ¡mara)
11. [CalibraciÃ³n del Sistema](#-calibraciÃ³n-del-sistema)
12. [EjecuciÃ³n del Sistema](#-ejecuciÃ³n-del-sistema)
13. [Troubleshooting](#-troubleshooting)
14. [CinemÃ¡tica del Robot](#-cinemÃ¡tica-del-robot)
15. [Plano de Planta](#-plano-de-planta)
16. [Conclusiones](#-conclusiones)
17. [Referencias](#-referencias)

---

## ğŸ“– IntroducciÃ³n

Este proyecto representa la culminaciÃ³n del curso de RobÃ³tica, integrando mÃºltiples disciplinas: cinemÃ¡tica de manipuladores, visiÃ³n por computadora, control de actuadores y desarrollo de software robÃ³tico. El sistema permite al robot **PhantomX Pincher X100** identificar, localizar y manipular objetos geomÃ©tricos de manera autÃ³noma.

---

## ğŸ¯ Objetivos

### Objetivo General

Desarrollar un sistema robÃ³tico completo de **Pick & Place** con visiÃ³n artificial que permita al robot PhantomX Pincher X100 detectar, clasificar y manipular figuras geomÃ©tricas de manera autÃ³noma utilizando una arquitectura distribuida de nodos ROS 2.

### Objetivos EspecÃ­ficos

1. **Implementar arquitectura distribuida ROS 2** con nodos independientes para visiÃ³n y control
2. **Desarrollar sistema de detecciÃ³n** con OpenCV capaz de identificar cÃ­rculos, cuadrados, rectÃ¡ngulos y pentÃ¡gonos
3. **Calcular coordenadas polares** de objetos respecto a disco de referencia
4. **Implementar cinemÃ¡tica inversa robusta** con algoritmo Levenberg-Marquardt
5. **Crear interfaz grÃ¡fica profesional** con PyQt5 para operaciÃ³n intuitiva
6. **Desarrollar rutinas automatizadas** de Pick & Place con clasificaciÃ³n por forma
7. **Integrar con MoveIt2** para planificaciÃ³n de trayectorias
8. **Documentar completamente** el sistema para replicabilidad

---

## ğŸ—ï¸ Arquitectura del Sistema

### Diagrama General de Componentes

```mermaid
flowchart TB
    subgraph HARDWARE["ğŸ”§ Hardware Layer"]
        CAM[ğŸ“· Logitech C270<br/>640Ã—480 @ 30fps]
        MOTORS[âš™ï¸ Dynamixel AX-12<br/>5 Servomotores]
        USB[ğŸ”Œ USB2Dynamixel]
        RPI[ğŸ‡ Raspberry Pi 5]
    end
    
    subgraph ROS2["ğŸ¤– ROS 2 Jazzy Layer"]
        direction TB
        
        subgraph VISION_NODE["opencv_detector_node"]
            CAPTURE[Captura Video]
            DETECT[Detecta Figuras]
            COORDS[Calcula Coordenadas]
            PUB[Publisher]
        end
        
        subgraph CONTROL_NODE["pincher_controller"]
            SUB[Subscriber]
            IK[CinemÃ¡tica Inversa]
            TRAJ[Planificador]
            MOTOR_CTRL[Control Motores]
        end
        
        subgraph ROS_CORE["ROS 2 Core"]
            TOPICS[TÃ³picos]
            RSP[robot_state_publisher]
            JSB[joint_state_broadcaster]
        end
    end
    
    subgraph GUI_LAYER["ğŸ’» User Interface"]
        GUI[GUI PyQt5]
        RVIZ[RViz2]
        TOOLBOX[Robotics Toolbox]
    end
    
    %% Conexiones Hardware
    CAM --> CAPTURE
    USB --> MOTORS
    RPI --> CAM
    RPI --> USB
    
    %% Flujo de VisiÃ³n
    CAPTURE --> DETECT
    DETECT --> COORDS
    COORDS --> PUB
    
    %% ComunicaciÃ³n ROS
    PUB --> TOPICS
    TOPICS --> SUB
    
    %% Flujo de Control
    SUB --> IK
    IK --> TRAJ
    TRAJ --> MOTOR_CTRL
    MOTOR_CTRL --> USB
    
    %% Estado del Robot
    MOTOR_CTRL --> JSB
    JSB --> RSP
    RSP --> TOPICS
    
    %% Interfaces de Usuario
    GUI --> CONTROL_NODE
    TOPICS --> RVIZ
    TOPICS --> TOOLBOX
```

### Diagrama de Nodos ROS 2

Basado en el grafo real del sistema (`rqt_graph`):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   opencv_detector_node  â”‚
â”‚      (Publisher)        â”‚
â”‚                         â”‚
â”‚  ğŸ“· Captura video       â”‚
â”‚  ğŸ” Detecta figuras     â”‚
â”‚  ğŸ“Š Calcula coords      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ Publica a:
            â”‚ â€¢ /opencv/image
            â”‚ â€¢ /opencv/shape
            â”‚ â€¢ /opencv/target_point
            â”‚ â€¢ /opencv/target_joint
            â”‚ â€¢ /opencv/detection_info
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   TÃ³picos    â”‚
     â”‚   ROS 2      â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ Suscripciones
            â”‚
            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   pincher_controller    â”‚
â”‚     (Subscriber)        â”‚
â”‚                         â”‚
â”‚  ğŸ¤– Control motores     â”‚
â”‚  ğŸ“ CinemÃ¡tica inversa  â”‚
â”‚  ğŸ¯ Pick & Place        â”‚
â”‚  ğŸ’» GUI PyQt5           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â”‚ Publica a:
            â”‚ â€¢ /joint_states
            â”‚ â€¢ /parameter_events
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ robot_state_     â”‚
     â”‚ publisher        â”‚
     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
            â–¼
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚   RViz2          â”‚
     â”‚   VisualizaciÃ³n  â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Flujo de Datos Detallado

```mermaid
sequenceDiagram
    participant CAM as ğŸ“· CÃ¡mara
    participant OCV as opencv_detector
    participant TOPIC as ğŸ”„ TÃ³picos ROS
    participant CTRL as pincher_controller
    participant GUI as ğŸ’» GUI
    participant ROBOT as ğŸ¤– Robot

    CAM->>OCV: Frame 640Ã—480
    OCV->>OCV: Detectar disco blanco
    OCV->>OCV: Detectar figura naranja
    OCV->>OCV: Calcular (r, Î¸)
    OCV->>OCV: Convertir a (x, y, z)
    
    OCV->>TOPIC: /opencv/image [Image]
    OCV->>TOPIC: /opencv/shape [String]
    OCV->>TOPIC: /opencv/target_point [PointStamped]
    OCV->>TOPIC: /opencv/detection_info [String/JSON]
    
    TOPIC->>CTRL: SuscripciÃ³n a tÃ³picos
    TOPIC->>GUI: Actualizar imagen
    
    GUI->>GUI: Usuario presiona<br/>"Guardar dato actual"
    GUI->>CTRL: Almacenar detecciÃ³n
    
    GUI->>GUI: Usuario presiona<br/>"Ejecutar rutina"
    GUI->>CTRL: Comando Pick & Place
    
    CTRL->>CTRL: CinemÃ¡tica inversa
    CTRL->>CTRL: Planificar trayectoria
    CTRL->>ROBOT: Mover a posiciÃ³n pick
    CTRL->>ROBOT: Cerrar gripper
    CTRL->>ROBOT: Mover a posiciÃ³n drop
    CTRL->>ROBOT: Abrir gripper
    
    CTRL->>TOPIC: /joint_states
    TOPIC->>GUI: Actualizar visualizaciÃ³n
```
<div align="center">
  <img src="./sources/rospraph_complete.png" alt="Ros Graph" width="1200"/>
  <p><em>Diagrama Denavit-Hartenberg del PhantomX Pincher X100</em></p>
</div>
---

## ğŸ“¥ InstalaciÃ³n y ConfiguraciÃ³n

### Requisitos del Sistema

| Componente | VersiÃ³n/EspecificaciÃ³n |
|:-----------|:-----------------------|
| Sistema Operativo | Ubuntu 24.04 LTS |
| ROS 2 | Jazzy Jalisco |
| Python | 3.12+ |
| OpenCV | 4.x |
| PyQt5 | 5.15+ |
| MoveIt2 | Jazzy |

### Paso 1: Instalar Dependencias ROS 2

```bash
# Actualizar sistema
sudo apt update && sudo apt upgrade -y

# ROS 2 Desktop Full
sudo apt install -y ros-jazzy-desktop-full

# Paquetes de control y planificaciÃ³n
sudo apt install -y \
    ros-jazzy-moveit \
    ros-jazzy-ros2-control \
    ros-jazzy-ros2-controllers \
    ros-jazzy-controller-manager \
    ros-jazzy-joint-state-broadcaster \
    ros-jazzy-joint-trajectory-controller

# Dynamixel SDK
sudo apt install -y ros-jazzy-dynamixel-sdk

# cv_bridge para conversiÃ³n de imÃ¡genes
sudo apt install -y ros-jazzy-cv-bridge

# Herramientas de visualizaciÃ³n
sudo apt install -y \
    ros-jazzy-rqt \
    ros-jazzy-rqt-image-view \
    ros-jazzy-rviz2
```

### Paso 2: Instalar Dependencias Python

```bash
# OpenCV y procesamiento de imÃ¡genes
pip3 install opencv-python numpy

# Interfaz grÃ¡fica
pip3 install PyQt5 PyQt5-sip

# CinemÃ¡tica y robÃ³tica
pip3 install roboticstoolbox-python spatialmath-python

# Dynamixel SDK Python
pip3 install dynamixel-sdk

# Utilidades adicionales
pip3 install scipy matplotlib
```

### Paso 3: Clonar y Compilar el Workspace

```bash
# Crear workspace
mkdir -p ~/ros2_ws/phantom_ws/src
cd ~/ros2_ws/phantom_ws/src

# Clonar repositorio
git clone https://github.com/samsanchezcar/Phantom_X_Pincher_ROS2_OpenCV.git .

# Volver al directorio del workspace
cd ~/ros2_ws/phantom_ws

# Compilar con colcon
colcon build

# Source el workspace
source install/setup.bash
```

---

## ğŸ“ Estructura de Paquetes

```
Phantom_X_Pincher_ROS2_OpenCV/
â”œâ”€â”€ README.md                              # Este archivo
â”œâ”€â”€ README_ROS2_OPENCV.md                  # DocumentaciÃ³n tÃ©cnica de arquitectura
â”œâ”€â”€ RESUMEN_ENTREGA.md                     # Resumen de cambios
â”œâ”€â”€ sources/                               # ğŸ“¸ Recursos multimedia
â”‚   â”œâ”€â”€ camara.png                        # CÃ¡mara utilizada
â”‚   â”œâ”€â”€ DH.png                            # Diagrama Denavit-Hartenberg
â”‚   â”œâ”€â”€ plano_inicial.jpg                 # Setup inicial
â”‚   â”œâ”€â”€ plano_final.jpg                   # DisposiciÃ³n de contenedores
â”‚   â””â”€â”€ gui/                              # Capturas de interfaz
â”‚
â”œâ”€â”€ phantomx_pincher_description/          # ğŸ“ DescripciÃ³n URDF/XACRO
â”‚   â”œâ”€â”€ urdf/
â”‚   â”‚   â”œâ”€â”€ phantomx_pincher.urdf.xacro  # Modelo principal
â”‚   â”‚   â”œâ”€â”€ _canastilla.urdf.xacro       # Canastilla de trabajo
â”‚   â”‚   â”œâ”€â”€ _mastil.urdf.xacro           # Soporte de cÃ¡mara
â”‚   â”‚   â””â”€â”€ _camara.urdf.xacro           # CÃ¡mara Logitech C270
â”‚   â”œâ”€â”€ meshes/                           # Modelos 3D STL
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ pincher_rviz.rviz            # ConfiguraciÃ³n RViz
â”‚   â”œâ”€â”€ launch/
â”‚   â”‚   â””â”€â”€ display.launch.py            # Launch para visualizaciÃ³n
â”‚   â”œâ”€â”€ package.xml
â”‚   â””â”€â”€ CMakeLists.txt
â”‚
â”œâ”€â”€ phantomx_pincher_moveit_config/        # ğŸ¯ ConfiguraciÃ³n MoveIt2
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ pincher.srdf                  # DefiniciÃ³n semÃ¡ntica
â”‚   â”‚   â”œâ”€â”€ joint_limits.yaml            # LÃ­mites de joints
â”‚   â”‚   â”œâ”€â”€ kinematics.yaml              # Solver de cinemÃ¡tica
â”‚   â”‚   â””â”€â”€ ompl_planning.yaml           # PlanificaciÃ³n OMPL
â”‚   â”œâ”€â”€ launch/
â”‚   â”‚   â”œâ”€â”€ moveit.launch.py
â”‚   â”‚   â””â”€â”€ demo.launch.py
â”‚   â”œâ”€â”€ package.xml
â”‚   â””â”€â”€ CMakeLists.txt
â”‚
â”œâ”€â”€ pincher_control/                       # ğŸ® Control y GUI
â”‚   â”œâ”€â”€ pincher_control/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ control_servo2.py            # â­ Controlador principal
â”‚   â”‚   â”œâ”€â”€ opencv_detector.py           # â­ Detector OpenCV
â”‚   â”‚   â”œâ”€â”€ terminal_control.py          # Control por CLI
â”‚   â”‚   â””â”€â”€ toolbox_viz.py               # VisualizaciÃ³n RTB
â”‚   â”œâ”€â”€ launch/
â”‚   â”‚   â””â”€â”€ pincher_opencv.launch.py     # Launch unificado
â”‚   â”œâ”€â”€ resource/
â”‚   â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ setup.py
â”‚   â”œâ”€â”€ setup.cfg
â”‚   â””â”€â”€ package.xml
â”‚
â”œâ”€â”€ phantomx_pincher_bringup/              # ğŸš€ Launch principal
â”‚   â”œâ”€â”€ launch/
â”‚   â”‚   â””â”€â”€ phantomx_pincher.launch.py   # Launch stack completo
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â””â”€â”€ pincher_controllers.yaml     # ConfiguraciÃ³n controladores
â”‚   â”œâ”€â”€ package.xml
â”‚   â””â”€â”€ CMakeLists.txt
â”‚
â””â”€â”€ demos/                                 # ğŸª Aplicaciones de demostraciÃ³n
    â”œâ”€â”€ demo_joystick.py
    â”œâ”€â”€ demo_trajectory.py
    â””â”€â”€ demo_pick_place.py
```

### Archivos Clave del Proyecto

| Archivo | DescripciÃ³n | Tipo |
|:--------|:------------|:-----|
| `control_servo2.py` | Nodo subscriber, controlador principal con GUI | Python Node |
| `opencv_detector.py` | Nodo publisher, detecciÃ³n de figuras | Python Node |
| `pincher_opencv.launch.py` | Launch file para ambos nodos | Launch File |
| `phantomx_pincher.urdf.xacro` | DescripciÃ³n URDF del robot | XACRO |
| `README_ROS2_OPENCV.md` | DocumentaciÃ³n tÃ©cnica detallada | Markdown |

---

## ğŸ‘ï¸ Sistema de VisiÃ³n OpenCV

### Nodo opencv_detector

**Tipo:** Publisher Node  
**Ejecutable:** `ros2 run pincher_control opencv_detector`  
**FunciÃ³n:** Captura video, detecta figuras geomÃ©tricas y publica informaciÃ³n a tÃ³picos ROS 2

#### InicializaciÃ³n del Nodo

```python
class OpenCVDetectorNode(Node):
    """
    Nodo ROS 2 para detecciÃ³n de figuras geomÃ©tricas con OpenCV.
    Captura video de la cÃ¡mara, detecta objetos naranjas sobre disco blanco,
    calcula coordenadas polares y publica a mÃºltiples tÃ³picos.
    """
    
    def __init__(self):
        super().__init__('opencv_detector_node')
        
        # Declarar parÃ¡metros configurables
        self.declare_parameter('camera_index', 0)
        self.declare_parameter('publish_rate', 10.0)
        
        # Obtener valores de parÃ¡metros
        camera_idx = self.get_parameter('camera_index').value
        rate_hz = self.get_parameter('publish_rate').value
        
        # Inicializar cÃ¡mara
        self.cap = cv2.VideoCapture(camera_idx)
        self.cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*"MJPG"))
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        
        # cv_bridge para conversiÃ³n de imÃ¡genes
        self.bridge = CvBridge()
        
        # Publishers
        self.image_pub = self.create_publisher(
            Image, '/opencv/image', 10
        )
        self.shape_pub = self.create_publisher(
            String, '/opencv/shape', 10
        )
        self.target_point_pub = self.create_publisher(
            PointStamped, '/opencv/target_point', 10
        )
        self.detection_info_pub = self.create_publisher(
            String, '/opencv/detection_info', 10
        )
        
        # Timer para captura y procesamiento
        self.timer = self.create_timer(
            1.0 / rate_hz, self.process_frame
        )
```

### Pipeline de DetecciÃ³n

#### 1. Captura de Frame

```python
def process_frame(self):
    """Captura y procesa un frame de la cÃ¡mara"""
    ret, frame = self.cap.read()
    if not ret:
        self.get_logger().warn("No se pudo capturar frame")
        return
    
    # Procesamiento...
```

#### 2. DetecciÃ³n del Disco Blanco

```python
def detect_white_disk(frame):
    """
    Detecta el disco blanco de referencia en la imagen.
    
    Returns:
        disk_center: (cx, cy) en pÃ­xeles
        disk_radius: radio en pÃ­xeles
    """
    # Convertir a escala de grises
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    
    # Umbral para blanco
    _, thresh = cv2.threshold(gray, 200, 255, cv2.THRESH_BINARY)
    
    # Encontrar contornos
    contours, _ = cv2.findContours(
        thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )
    
    # Filtrar por circularidad y Ã¡rea
    for cnt in contours:
        area = cv2.contourArea(cnt)
        if area < 5000:  # Filtro de Ã¡rea mÃ­nima
            continue
            
        # Ajustar cÃ­rculo mÃ­nimo
        (x, y), radius = cv2.minEnclosingCircle(cnt)
        circle_area = np.pi * radius**2
        circularity = area / circle_area
        
        if circularity > 0.8:  # Es un cÃ­rculo
            return (int(x), int(y)), int(radius)
    
    return None, None
```

#### 3. DetecciÃ³n de Figuras Naranjas

```python
def detect_orange_shapes(frame, disk_center, disk_radius):
    """
    Detecta figuras naranjas en el Ã¡rea del disco.
    
    Returns:
        shape_code: 's', 'r', 'c', 'p', 'u'
        shape_name: nombre completo de la figura
        center_px: centro de la figura en pÃ­xeles
        confidence: confianza de la detecciÃ³n (0-1)
    """
    # ROI alrededor del disco
    x, y = disk_center
    r = disk_radius
    roi = frame[max(0, y-r):y+r, max(0, x-r):x+r]
    
    # Convertir a HSV
    hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)
    
    # Rango para naranja
    lower_orange = np.array([3, 70, 70])
    upper_orange = np.array([28, 255, 255])
    
    # MÃ¡scara
    mask = cv2.inRange(hsv, lower_orange, upper_orange)
    
    # Operaciones morfolÃ³gicas
    kernel = np.ones((5, 5), np.uint8)
    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)
    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)
    
    # Encontrar contornos
    contours, _ = cv2.findContours(
        mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
    )
    
    if not contours:
        return 'u', 'unknown', None, 0.0
    
    # Encontrar el contorno mÃ¡s grande
    largest_cnt = max(contours, key=cv2.contourArea)
    area = cv2.contourArea(largest_cnt)
    
    if area < 500:  # Filtro de Ã¡rea mÃ­nima
        return 'u', 'unknown', None, 0.0
    
    # Clasificar forma
    shape_code, shape_name, conf = classify_shape(largest_cnt)
    
    # Calcular centro
    M = cv2.moments(largest_cnt)
    if M['m00'] != 0:
        cx = int(M['m10'] / M['m00']) + max(0, x-r)
        cy = int(M['m01'] / M['m00']) + max(0, y-r)
        center_px = (cx, cy)
    else:
        center_px = None
    
    return shape_code, shape_name, center_px, conf
```

#### 4. ClasificaciÃ³n de Formas

Sistema de scoring multicriterio para clasificaciÃ³n robusta:

```python
def classify_shape(contour):
    """
    Clasifica una forma geomÃ©trica usando scoring multicriterio.
    
    Criterios:
    - NÃºmero de vÃ©rtices (aproximaciÃ³n poligonal)
    - Circularidad (Ã¡rea / Ã¡rea del cÃ­rculo inscrito)
    - Aspect ratio (relaciÃ³n ancho/alto)
    - Solidez (Ã¡rea / Ã¡rea convexa)
    
    Returns:
        code: 's' (cuadrado), 'r' (rectÃ¡ngulo), 'c' (cÃ­rculo), 'p' (pentÃ¡gono)
        name: nombre completo
        confidence: confianza 0-1
    """
    # AproximaciÃ³n poligonal
    epsilon = 0.04 * cv2.arcLength(contour, True)
    approx = cv2.approxPolyDP(contour, epsilon, True)
    vertices = len(approx)
    
    # CaracterÃ­sticas geomÃ©tricas
    area = cv2.contourArea(contour)
    perimeter = cv2.arcLength(contour, True)
    (x, y), radius = cv2.minEnclosingCircle(contour)
    
    # Circularidad
    circularity = (4 * np.pi * area) / (perimeter ** 2) if perimeter > 0 else 0
    
    # Aspect ratio
    rect = cv2.minAreaRect(contour)
    width, height = rect[1]
    aspect_ratio = max(width, height) / min(width, height) if min(width, height) > 0 else 0
    
    # Solidez
    hull = cv2.convexHull(contour)
    hull_area = cv2.contourArea(hull)
    solidity = area / hull_area if hull_area > 0 else 0
    
    # Sistema de scoring
    scores = {
        'circle': 0,
        'square': 0,
        'rectangle': 0,
        'pentagon': 0
    }
    
    # Scoring por vÃ©rtices
    if vertices == 5:
        scores['pentagon'] += 3.0
    elif vertices == 4:
        if 0.85 < aspect_ratio < 1.15:
            scores['square'] += 2.5
        else:
            scores['rectangle'] += 2.5
    elif vertices > 8:
        scores['circle'] += 3.0
    
    # Scoring por circularidad
    if circularity > 0.85:
        scores['circle'] += 2.0
    elif 0.7 < circularity < 0.85:
        scores['pentagon'] += 1.5
    elif 0.5 < circularity < 0.7:
        if 0.85 < aspect_ratio < 1.15:
            scores['square'] += 1.5
        else:
            scores['rectangle'] += 1.5
    
    # Scoring por aspect ratio
    if 0.9 < aspect_ratio < 1.1:
        scores['circle'] += 1.0
        scores['square'] += 1.0
    elif aspect_ratio > 1.3:
        scores['rectangle'] += 2.0
    
    # Determinar ganador
    best_shape = max(scores, key=scores.get)
    max_score = scores[best_shape]
    total_possible = 6.0
    confidence = min(max_score / total_possible, 1.0)
    
    # Mapeo a cÃ³digos
    code_map = {
        'circle': ('c', 'circle'),
        'square': ('s', 'square'),
        'rectangle': ('r', 'rectangle'),
        'pentagon': ('p', 'pentagon')
    }
    
    code, name = code_map[best_shape]
    return code, name, confidence
```

### CÃ¡lculo de Coordenadas

#### Coordenadas Polares

```python
def calculate_polar_coords(shape_center, disk_center, disk_radius_px):
    """
    Calcula coordenadas polares (r, Î¸) respecto al centro del disco.
    
    Args:
        shape_center: (x, y) en pÃ­xeles de la figura
        disk_center: (x, y) en pÃ­xeles del disco
        disk_radius_px: radio del disco en pÃ­xeles
    
    Returns:
        r_cm: distancia radial en centÃ­metros
        theta_deg: Ã¡ngulo en grados (0Â° = derecha, sentido antihorario)
    """
    # Radio fÃ­sico del disco (medido)
    DISK_RADIUS_CM = 7.25
    
    # Factor de escala pÃ­xeles â†’ cm
    scale = DISK_RADIUS_CM / disk_radius_px
    
    # Vector del disco al objeto
    dx = shape_center[0] - disk_center[0]
    dy = shape_center[1] - disk_center[1]
    
    # Distancia euclidiana en pÃ­xeles
    r_px = np.sqrt(dx**2 + dy**2)
    
    # Convertir a centÃ­metros
    r_cm = r_px * scale
    
    # Ãngulo en radianes (atan2 maneja correctamente los cuadrantes)
    theta_rad = np.arctan2(-dy, dx)  # -dy porque Y crece hacia abajo
    
    # Convertir a grados
    theta_deg = np.degrees(theta_rad)
    
    # Normalizar a [0, 360)
    if theta_deg < 0:
        theta_deg += 360
    
    return r_cm, theta_deg
```

#### Coordenadas Cartesianas 3D

```python
def polar_to_cartesian_3d(r_cm, theta_deg):
    """
    Convierte coordenadas polares a cartesianas 3D en el frame del robot.
    
    Args:
        r_cm: distancia radial en cm
        theta_deg: Ã¡ngulo en grados
    
    Returns:
        x_m, y_m, z_m: coordenadas en metros
    """
    # Offsets de calibraciÃ³n cÃ¡mara-robot (ajustar segÃºn setup)
    OPENCV_OFFSET_X_M = -0.1  # offset en X
    OPENCV_OFFSET_Y_M = 0.0   # offset en Y
    
    # Convertir cm a metros
    r_m = r_cm / 100.0
    
    # Convertir grados a radianes
    theta_rad = np.radians(theta_deg)
    
    # Coordenadas cartesianas
    x_raw = r_m * np.cos(theta_rad)
    y_raw = r_m * np.sin(theta_rad)
    
    # Aplicar offsets de calibraciÃ³n
    x_m = x_raw + OPENCV_OFFSET_X_M
    y_m = y_raw + OPENCV_OFFSET_Y_M
    
    # Z fijo (altura de la mesa)
    z_m = -0.025  # 2.5 cm bajo el origen del robot
    
    return x_m, y_m, z_m
```

### PublicaciÃ³n a TÃ³picos ROS 2

```python
def publish_detection(self, frame, shape_code, shape_name, 
                     r_cm, theta_deg, x_m, y_m, z_m, confidence):
    """
    Publica detecciÃ³n a todos los tÃ³picos ROS 2.
    """
    timestamp = self.get_clock().now().to_msg()
    
    # 1. Imagen procesada
    try:
        img_msg = self.bridge.cv2_to_imgmsg(frame, encoding='bgr8')
        img_msg.header.stamp = timestamp
        img_msg.header.frame_id = 'camera_optical_frame'
        self.image_pub.publish(img_msg)
    except Exception as e:
        self.get_logger().error(f'Error publicando imagen: {e}')
    
    # 2. CÃ³digo de forma
    shape_msg = String()
    shape_msg.data = shape_code
    self.shape_pub.publish(shape_msg)
    
    # 3. Punto 3D
    point_msg = PointStamped()
    point_msg.header.stamp = timestamp
    point_msg.header.frame_id = 'disk_frame'
    point_msg.point.x = x_m
    point_msg.point.y = y_m
    point_msg.point.z = z_m
    self.target_point_pub.publish(point_msg)
    
    # 4. InformaciÃ³n completa en JSON
    detection_dict = {
        'shape_name': shape_name,
        'shape_code': shape_code,
        'confidence': round(confidence, 3),
        'r_cm': round(r_cm, 2),
        'theta_deg': round(theta_deg, 1),
        'x_m': round(x_m, 4),
        'y_m': round(y_m, 4),
        'z_m': round(z_m, 4),
        'timestamp': timestamp.sec + timestamp.nanosec * 1e-9
    }
    
    info_msg = String()
    info_msg.data = json.dumps(detection_dict)
    self.detection_info_pub.publish(info_msg)
    
    # Log
    self.get_logger().info(
        f'Detectado: {shape_name} ({shape_code}) | '
        f'r={r_cm:.2f}cm Î¸={theta_deg:.1f}Â° | '
        f'XYZ=({x_m:.3f}, {y_m:.3f}, {z_m:.3f})m'
    )
```

### ParÃ¡metros Configurables

```bash
# Cambiar Ã­ndice de cÃ¡mara (default: 0)
ros2 run pincher_control opencv_detector --ros-args -p camera_index:=1

# Cambiar frecuencia de publicaciÃ³n (default: 10 Hz)
ros2 run pincher_control opencv_detector --ros-args -p publish_rate:=30.0
```

---

## ğŸ® Controlador Principal

### Nodo pincher_controller

**Tipo:** Subscriber Node + GUI  
**Ejecutable:** `ros2 run pincher_control control_servo`  
**FunciÃ³n:** Controla el robot, se suscribe a detecciones OpenCV y proporciona interfaz grÃ¡fica

#### InicializaciÃ³n del Nodo

```python
class PincherController(Node):
    """
    Controlador principal del PhantomX Pincher con integraciÃ³n OpenCV.
    
    CaracterÃ­sticas:
    - Control de 5 motores Dynamixel AX-12
    - CinemÃ¡tica directa e inversa
    - SuscripciÃ³n a tÃ³picos de OpenCV
    - PublicaciÃ³n de estados del robot
    - Rutinas automatizadas de Pick & Place
    """
    
    def __init__(self):
        super().__init__('pincher_controller')
        
        # ParÃ¡metros configurables
        self.declare_parameter('port', '/dev/ttyUSB0')
        self.declare_parameter('baudrate', 1000000)
        
        # Inicializar comunicaciÃ³n con Dynamixel
        self.init_dynamixel()
        
        # Suscripciones a tÃ³picos OpenCV
        self.image_sub = self.create_subscription(
            Image, '/opencv/image', 
            self.image_callback, 10
        )
        self.shape_sub = self.create_subscription(
            String, '/opencv/shape',
            self.shape_callback, 10
        )
        self.target_point_sub = self.create_subscription(
            PointStamped, '/opencv/target_point',
            self.target_point_callback, 10
        )
        self.detection_info_sub = self.create_subscription(
            String, '/opencv/detection_info',
            self.detection_info_callback, 10
        )
        
        # Publisher para estado del robot
        self.joint_state_pub = self.create_publisher(
            JointState, '/joint_states', 10
        )
        
        # Variables de estado
        self.last_opencv_image = None
        self.last_shape_code = 'u'
        self.last_target_point = None
        self.saved_detection = None
        
        # Timer para publicar estados
        self.state_timer = self.create_timer(0.1, self.publish_joint_states)
```

#### Callbacks de SuscripciÃ³n

```python
def image_callback(self, msg):
    """Recibe imagen procesada de OpenCV"""
    try:
        cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')
        self.last_opencv_image = cv_image
        # La imagen se muestra en la GUI automÃ¡ticamente
    except Exception as e:
        self.get_logger().error(f'Error convirtiendo imagen: {e}')

def shape_callback(self, msg):
    """Recibe cÃ³digo de forma detectada"""
    self.last_shape_code = msg.data
    self.get_logger().debug(f'Forma recibida: {msg.data}')

def target_point_callback(self, msg):
    """Recibe punto 3D del target"""
    self.last_target_point = {
        'x': msg.point.x,
        'y': msg.point.y,
        'z': msg.point.z
    }

def detection_info_callback(self, msg):
    """Recibe informaciÃ³n completa de detecciÃ³n en JSON"""
    try:
        detection = json.loads(msg.data)
        self.last_full_detection = detection
        self.get_logger().info(
            f"DetecciÃ³n completa: {detection['shape_name']} "
            f"en ({detection['x_m']:.3f}, {detection['y_m']:.3f}, {detection['z_m']:.3f})"
        )
    except json.JSONDecodeError as e:
        self.get_logger().error(f'Error parseando JSON: {e}')
```

### CinemÃ¡tica Inversa

#### Modelo CinemÃ¡tico

ParÃ¡metros Denavit-Hartenberg del PhantomX Pincher:

| Joint | Î¸ (rad) | d (m) | a (m) | Î± (rad) | LÃ­mites (Â°) |
|:------|:--------|:------|:------|:--------|:-----------|
| 1 | q1 | 0.067 | 0 | Ï€/2 | [-150, 150] |
| 2 | q2 - Ï€/2 | 0 | 0.105 | 0 | [-110, 110] |
| 3 | q3 | 0 | 0.105 | 0 | [-110, 110] |
| 4 | q4 | 0 | 0.110 | 0 | [-110, 110] |

#### Algoritmo Levenberg-Marquardt

```python
def inverse_kinematics_lm(target_pos, initial_guess=None, max_iterations=100):
    """
    Calcula cinemÃ¡tica inversa usando Levenberg-Marquardt.
    
    Args:
        target_pos: [x, y, z] posiciÃ³n deseada del end-effector (m)
        initial_guess: [q1, q2, q3, q4] Ã¡ngulos iniciales (rad)
        max_iterations: nÃºmero mÃ¡ximo de iteraciones
    
    Returns:
        joint_angles: [q1, q2, q3, q4] soluciÃ³n (rad)
        success: True si convergiÃ³
    """
    # ConfiguraciÃ³n
    lambda_param = 0.01  # Factor de Levenberg-Marquardt
    tolerance = 1e-4      # Tolerancia de error
    
    # Guess inicial (si no se proporciona)
    if initial_guess is None:
        initial_guess = np.array([0.0, 0.0, 0.0, 0.0])
    
    q = np.array(initial_guess)
    
    for iteration in range(max_iterations):
        # Forward kinematics con q actual
        T = forward_kinematics(q)
        current_pos = T[0:3, 3]
        
        # Error
        error = target_pos - current_pos
        error_norm = np.linalg.norm(error)
        
        # Verificar convergencia
        if error_norm < tolerance:
            return q, True
        
        # Jacobiano geomÃ©trico
        J = geometric_jacobian(q)
        J_pos = J[0:3, :]  # Solo componente posicional
        
        # MÃ©todo Levenberg-Marquardt
        JtJ = J_pos.T @ J_pos
        identity = np.eye(4)
        damped_inverse = np.linalg.inv(JtJ + lambda_param * identity)
        
        # ActualizaciÃ³n de joints
        delta_q = damped_inverse @ J_pos.T @ error
        q = q + delta_q
        
        # Aplicar lÃ­mites de joints
        q = np.clip(q, JOINT_LIMITS_MIN, JOINT_LIMITS_MAX)
        
        # Ajustar lambda adaptivamente
        if error_norm < prev_error_norm:
            lambda_param *= 0.9  # Disminuir damping
        else:
            lambda_param *= 1.5  # Aumentar damping
        
        prev_error_norm = error_norm
    
    return q, False  # No convergiÃ³

def geometric_jacobian(q):
    """
    Calcula el Jacobiano geomÃ©trico en configuraciÃ³n q.
    
    Returns:
        J: Matriz 6Ã—4 (velocidad lineal + angular)
    """
    # ImplementaciÃ³n basada en producto de exponenciales
    # o derivaciÃ³n numÃ©rica de FK
    ...
    
    return J
```

#### Estrategia Multi-Seed

Para robustez, se prueban mÃºltiples configuraciones iniciales:

```python
def solve_ik_robust(target_xyz):
    """
    Intenta resolver IK con mÃºltiples semillas.
    
    Returns:
        best_solution: mejor configuraciÃ³n encontrada
        success: True si alguna semilla convergiÃ³
    """
    # Semillas a probar
    seeds = [
        [0.0, 0.0, 0.0, 0.0],           # ConfiguraciÃ³n neutra
        [0.0, np.pi/4, -np.pi/4, 0.0],  # Hacia adelante
        [np.pi/2, 0.0, 0.0, 0.0],       # Lateral derecha
        [-np.pi/2, 0.0, 0.0, 0.0],      # Lateral izquierda
        [0.0, -np.pi/6, np.pi/3, 0.0],  # Hacia arriba
    ]
    
    best_solution = None
    best_error = float('inf')
    
    for seed in seeds:
        solution, success = inverse_kinematics_lm(target_xyz, seed)
        
        if success:
            # Verificar error final
            T = forward_kinematics(solution)
            achieved_pos = T[0:3, 3]
            error = np.linalg.norm(target_xyz - achieved_pos)
            
            if error < best_error:
                best_error = error
                best_solution = solution
    
    return best_solution, (best_error < 0.01)
```

### Rutina Pick & Place

#### ConfiguraciÃ³n de Puntos de Destino

```python
# Puntos de drop por forma (x, y, z) en metros
DROP_POINTS = {
    's': (0.15, -0.15, -0.02),   # Cuadrado â†’ contenedor rojo
    'r': (0.15, 0.15, -0.02),    # RectÃ¡ngulo â†’ contenedor verde
    'c': (0.20, 0.0, -0.02),     # CÃ­rculo â†’ contenedor azul
    'p': (0.12, 0.0, -0.02),     # PentÃ¡gono â†’ contenedor amarillo
}

# Altura de aproximaciÃ³n (sobre el objeto)
APPROACH_HEIGHT = 0.05  # 5 cm sobre el objeto

# Gripper IDs
GRIPPER_ID = 5
GRIPPER_OPEN = 512    # PosiciÃ³n abierto
GRIPPER_CLOSED = 680  # PosiciÃ³n cerrado
```

#### Secuencia de EjecuciÃ³n

```python
def execute_pick_and_place(self, detection):
    """
    Ejecuta secuencia completa de Pick & Place.
    
    Args:
        detection: dict con 'shape_code', 'x_m', 'y_m', 'z_m'
    
    Steps:
        1. Home position
        2. Approach (sobre objeto)
        3. Descend (bajar al objeto)
        4. Grasp (cerrar gripper)
        5. Lift (levantar objeto)
        6. Move to drop zone
        7. Release (abrir gripper)
        8. Return home
    """
    shape_code = detection['shape_code']
    
    if shape_code not in DROP_POINTS:
        self.get_logger().error(f'Forma desconocida: {shape_code}')
        return False
    
    # Posiciones clave
    pick_pos = np.array([
        detection['x_m'],
        detection['y_m'],
        detection['z_m']
    ])
    
    approach_pos = pick_pos + np.array([0, 0, APPROACH_HEIGHT])
    drop_pos = np.array(DROP_POINTS[shape_code])
    drop_approach = drop_pos + np.array([0, 0, APPROACH_HEIGHT])
    
    try:
        # Paso 1: Home position
        self.get_logger().info('Step 1/8: Moving to home')
        self.move_to_home_opencv()
        time.sleep(1.0)
        
        # Paso 2: Open gripper
        self.get_logger().info('Step 2/8: Opening gripper')
        self.set_gripper_position(GRIPPER_OPEN)
        time.sleep(0.5)
        
        # Paso 3: Approach position
        self.get_logger().info('Step 3/8: Approaching object')
        success = self.move_to_xyz(approach_pos)
        if not success:
            raise Exception("Failed to reach approach position")
        time.sleep(0.5)
        
        # Paso 4: Descend to object
        self.get_logger().info('Step 4/8: Descending to object')
        success = self.move_to_xyz(pick_pos)
        if not success:
            raise Exception("Failed to reach pick position")
        time.sleep(0.5)
        
        # Paso 5: Grasp
        self.get_logger().info('Step 5/8: Grasping object')
        self.set_gripper_position(GRIPPER_CLOSED)
        time.sleep(1.0)
        
        # Paso 6: Lift
        self.get_logger().info('Step 6/8: Lifting object')
        success = self.move_to_xyz(approach_pos)
        if not success:
            raise Exception("Failed to lift object")
        time.sleep(0.5)
        
        # Paso 7: Move to drop approach
        self.get_logger().info('Step 7/8: Moving to drop zone')
        success = self.move_to_xyz(drop_approach)
        if not success:
            raise Exception("Failed to reach drop approach")
        time.sleep(0.5)
        
        # Paso 8: Descend to drop
        success = self.move_to_xyz(drop_pos)
        if not success:
            raise Exception("Failed to reach drop position")
        time.sleep(0.5)
        
        # Paso 9: Release
        self.get_logger().info('Step 8/8: Releasing object')
        self.set_gripper_position(GRIPPER_OPEN)
        time.sleep(0.5)
        
        # Paso 10: Lift from drop
        success = self.move_to_xyz(drop_approach)
        time.sleep(0.5)
        
        # Paso 11: Return home
        self.get_logger().info('Returning to home')
        self.move_to_home_opencv()
        
        self.get_logger().info('âœ“ Pick & Place completed successfully!')
        return True
        
    except Exception as e:
        self.get_logger().error(f'Pick & Place failed: {e}')
        # Emergency: open gripper and return home
        self.set_gripper_position(GRIPPER_OPEN)
        self.move_to_home_opencv()
        return False
```

#### Movimiento Cartesiano Seguro

```python
def move_to_xyz(self, target_xyz, validate=True):
    """
    Mueve el end-effector a posiciÃ³n cartesiana.
    
    Args:
        target_xyz: [x, y, z] en metros
        validate: verificar zona de trabajo
    
    Returns:
        success: True si movimiento se completÃ³
    """
    # Validar zona de trabajo
    if validate and not self.is_position_safe(target_xyz):
        self.get_logger().error(
            f'PosiciÃ³n no segura: ({target_xyz[0]:.3f}, '
            f'{target_xyz[1]:.3f}, {target_xyz[2]:.3f})'
        )
        return False
    
    # Resolver cinemÃ¡tica inversa
    joint_angles, success = self.solve_ik_robust(target_xyz)
    
    if not success:
        self.get_logger().error('IK no convergiÃ³')
        return False
    
    # Mover motores
    for i, angle_rad in enumerate(joint_angles):
        motor_id = i + 1
        position_value = self.rad_to_dynamixel(angle_rad, motor_id)
        self.write_position(motor_id, position_value)
    
    # Esperar a que alcance posiciÃ³n
    time.sleep(0.5)
    
    return True

def is_position_safe(self, xyz):
    """
    Verifica si una posiciÃ³n estÃ¡ en zona de trabajo segura.
    
    Restricciones:
    - Distancia radial mÃ­nima/mÃ¡xima
    - Altura mÃ­nima (no colisionar con mesa)
    - Evitar singularidades
    """
    x, y, z = xyz
    
    # Distancia radial
    r = np.sqrt(x**2 + y**2)
    if r < 0.05 or r > 0.30:  # 5cm - 30cm
        return False
    
    # Altura mÃ­nima
    if z < -0.05:  # No mÃ¡s de 5cm bajo origen
        return False
    
    # Evitar singularidad en eje Z
    if abs(x) < 0.02 and abs(y) < 0.02:
        return False
    
    return True
```

---

## ğŸ’» Interfaz GrÃ¡fica (GUI)

### Arquitectura de la GUI

La GUI estÃ¡ desarrollada en PyQt5 con diseÃ±o moderno y organizada en 8 pestaÃ±as especializadas:

```python
class ModernPincherGUI(QMainWindow):
    """
    Interfaz grÃ¡fica principal del sistema PhantomX Pincher.
    
    PestaÃ±as:
    1. ğŸ  Inicio - Resumen y quick actions
    2. ğŸ® Control Manual - Sliders para cada motor
    3. ğŸ“ Control XYZ - Control cartesiano con IK
    4. ğŸ”§ Valores Fijos - Posiciones predefinidas
    5. ğŸ“· OpenCV - VisualizaciÃ³n y control de visiÃ³n
    6. ğŸ¤– Rutinas - EjecuciÃ³n de secuencias
    7. ğŸ“Š Monitoreo - Estado del robot en tiempo real
    8. âš™ï¸ ConfiguraciÃ³n - ParÃ¡metros del sistema
    """
```

### PestaÃ±a OpenCV (Clave)

```python
def setup_opencv_tab(self):
    """Configura la pestaÃ±a de OpenCV con imagen y controles"""
    layout = QVBoxLayout()
    
    # TÃ­tulo
    title = QLabel("ğŸ“· Sistema de VisiÃ³n OpenCV")
    title.setStyleSheet("font-size: 24px; font-weight: bold;")
    layout.addWidget(title)
    
    # Widget de imagen
    self.opencv_image_label = QLabel()
    self.opencv_image_label.setMinimumSize(640, 480)
    self.opencv_image_label.setScaledContents(True)
    self.opencv_image_label.setStyleSheet(
        "border: 2px solid #3498db; border-radius: 10px;"
    )
    layout.addWidget(self.opencv_image_label)
    
    # Estado de detecciÃ³n
    self.detection_status = QLabel("â³ Esperando imagen de /opencv/image...")
    self.detection_status.setStyleSheet(
        "font-size: 14px; padding: 10px; "
        "background-color: #f39c12; color: white; border-radius: 5px;"
    )
    layout.addWidget(self.detection_status)
    
    # Controles
    button_layout = QHBoxLayout()
    
    # BotÃ³n: Guardar detecciÃ³n
    save_btn = QPushButton("ğŸ“Œ Guardar dato actual")
    save_btn.setStyleSheet(self.get_button_style("#27ae60"))
    save_btn.clicked.connect(self.save_current_detection)
    button_layout.addWidget(save_btn)
    
    # BotÃ³n: Ejecutar rutina
    execute_btn = QPushButton("ğŸ¤– Ejecutar rutina")
    execute_btn.setStyleSheet(self.get_button_style("#e74c3c"))
    execute_btn.clicked.connect(self.execute_opencv_routine)
    button_layout.addWidget(execute_btn)
    
    # BotÃ³n: Home OpenCV
    home_btn = QPushButton("ğŸ  Home OpenCV")
    home_btn.setStyleSheet(self.get_button_style("#3498db"))
    home_btn.clicked.connect(self.controller.move_to_home_opencv)
    button_layout.addWidget(home_btn)
    
    layout.addLayout(button_layout)
    
    # InformaciÃ³n de Ãºltima detecciÃ³n guardada
    self.saved_detection_info = QTextEdit()
    self.saved_detection_info.setReadOnly(True)
    self.saved_detection_info.setMaximumHeight(150)
    self.saved_detection_info.setPlaceholderText(
        "No hay detecciÃ³n guardada. Presiona 'Guardar dato actual' "
        "cuando se detecte una figura."
    )
    layout.addWidget(self.saved_detection_info)
    
    # Timer para actualizar imagen
    self.opencv_update_timer = QTimer()
    self.opencv_update_timer.timeout.connect(self.update_opencv_image)
    self.opencv_update_timer.start(100)  # 10 Hz
    
    opencv_tab = QWidget()
    opencv_tab.setLayout(layout)
    return opencv_tab
```

### MÃ©todos de InteracciÃ³n

```python
def update_opencv_image(self):
    """Actualiza la imagen de OpenCV en la GUI"""
    if self.controller.last_opencv_image is not None:
        frame = self.controller.last_opencv_image
        
        # Convertir OpenCV (BGR) a Qt (RGB)
        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        h, w, ch = rgb_frame.shape
        bytes_per_line = ch * w
        
        qt_image = QImage(
            rgb_frame.data, w, h, bytes_per_line, QImage.Format_RGB888
        )
        pixmap = QPixmap.fromImage(qt_image)
        
        self.opencv_image_label.setPixmap(pixmap)
        
        # Actualizar estado
        shape_code = self.controller.last_shape_code
        shape_names = {
            's': 'Cuadrado',
            'r': 'RectÃ¡ngulo',
            'c': 'CÃ­rculo',
            'p': 'PentÃ¡gono',
            'u': 'Desconocido'
        }
        shape_name = shape_names.get(shape_code, 'Desconocido')
        
        if shape_code != 'u':
            self.detection_status.setText(
                f"âœ… Detectado: {shape_name} ({shape_code})"
            )
            self.detection_status.setStyleSheet(
                "font-size: 14px; padding: 10px; "
                "background-color: #27ae60; color: white; border-radius: 5px;"
            )
        else:
            self.detection_status.setText(
                "â³ Buscando figuras naranjas..."
            )
            self.detection_status.setStyleSheet(
                "font-size: 14px; padding: 10px; "
                "background-color: #f39c12; color: white; border-radius: 5px;"
            )
    else:
        self.detection_status.setText(
            "âŒ Sin imagen de /opencv/image"
        )
        self.detection_status.setStyleSheet(
            "font-size: 14px; padding: 10px; "
            "background-color: #e74c3c; color: white; border-radius: 5px;"
        )

def save_current_detection(self):
    """Guarda la detecciÃ³n actual para uso posterior"""
    if self.controller.last_full_detection is None:
        QMessageBox.warning(
            self, 
            "Sin DetecciÃ³n",
            "No hay ninguna detecciÃ³n disponible para guardar."
        )
        return
    
    # Guardar detecciÃ³n
    self.controller.saved_detection = self.controller.last_full_detection
    
    # Mostrar informaciÃ³n
    det = self.controller.saved_detection
    info_text = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘      DETECCIÃ“N GUARDADA              â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“ Forma: {det['shape_name'].upper()}
ğŸ”¤ CÃ³digo: {det['shape_code']}
ğŸ“Š Confianza: {det['confidence']*100:.1f}%

ğŸ“ Coordenadas Polares:
   â€¢ r = {det['r_cm']:.2f} cm
   â€¢ Î¸ = {det['theta_deg']:.1f}Â°

ğŸ“ Coordenadas Cartesianas:
   â€¢ X = {det['x_m']:.4f} m
   â€¢ Y = {det['y_m']:.4f} m
   â€¢ Z = {det['z_m']:.4f} m

ğŸ• Timestamp: {det['timestamp']:.2f}

âœ… Listo para ejecutar rutina de Pick & Place
    """
    
    self.saved_detection_info.setText(info_text)
    
    QMessageBox.information(
        self,
        "DetecciÃ³n Guardada",
        f"Se guardÃ³ detecciÃ³n de {det['shape_name']} en "
        f"({det['x_m']:.3f}, {det['y_m']:.3f}, {det['z_m']:.3f})m"
    )

def execute_opencv_routine(self):
    """Ejecuta rutina de Pick & Place con dato guardado"""
    if self.controller.saved_detection is None:
        QMessageBox.warning(
            self,
            "Sin Dato Guardado",
            "Primero debes guardar una detecciÃ³n presionando "
            "'Guardar dato actual'."
        )
        return
    
    # Confirmar ejecuciÃ³n
    det = self.controller.saved_detection
    reply = QMessageBox.question(
        self,
        "Confirmar EjecuciÃ³n",
        f"Â¿Ejecutar Pick & Place para {det['shape_name']} en "
        f"({det['x_m']:.3f}, {det['y_m']:.3f}, {det['z_m']:.3f})m?",
        QMessageBox.Yes | QMessageBox.No
    )
    
    if reply == QMessageBox.Yes:
        # Ejecutar en thread separado para no bloquear GUI
        success = self.controller.execute_pick_and_place(
            self.controller.saved_detection
        )
        
        if success:
            QMessageBox.information(
                self,
                "Rutina Completada",
                "La rutina de Pick & Place se ejecutÃ³ correctamente."
            )
        else:
            QMessageBox.critical(
                self,
                "Rutina Fallida",
                "Hubo un error ejecutando la rutina. "
                "Revisa los logs para mÃ¡s informaciÃ³n."
            )
```

### DiseÃ±o Visual

La GUI utiliza una paleta de colores moderna y consistente:

```python
def get_button_style(self, color):
    """Retorna estilo CSS para botones"""
    return f"""
        QPushButton {{
            background-color: {color};
            color: white;
            border: none;
            padding: 12px 24px;
            font-size: 14px;
            font-weight: bold;
            border-radius: 6px;
        }}
        QPushButton:hover {{
            background-color: {self.lighten_color(color, 20)};
        }}
        QPushButton:pressed {{
            background-color: {self.darken_color(color, 20)};
        }}
    """

# Paleta de colores
COLORS = {
    'primary': '#3498db',    # Azul
    'success': '#27ae60',    # Verde
    'danger': '#e74c3c',     # Rojo
    'warning': '#f39c12',    # Naranja
    'info': '#1abc9c',       # Turquesa
    'dark': '#2c3e50',       # Gris oscuro
}
```

---

## ğŸ”„ Nodos y TÃ³picos ROS 2

### Nodos del Sistema

| Nodo | Tipo | FunciÃ³n | Ejecutable |
|:-----|:-----|:--------|:-----------|
| `opencv_detector_node` | Publisher | DetecciÃ³n de figuras y publicaciÃ³n | `opencv_detector` |
| `pincher_controller` | Subscriber + Publisher | Control del robot y GUI | `control_servo` |
| `robot_state_publisher` | Publisher | Publica TF tree del robot | ROS2 Core |
| `joint_state_broadcaster` | Broadcaster | Difunde estados de joints | ROS2 Controllers |

### TÃ³picos Publicados

#### Por opencv_detector_node:

| TÃ³pico | Tipo | Frecuencia | DescripciÃ³n |
|:-------|:-----|:-----------|:------------|
| `/opencv/image` | `sensor_msgs/Image` | 10 Hz | Imagen procesada 640Ã—480 con overlays |
| `/opencv/shape` | `std_msgs/String` | 10 Hz | CÃ³digo de forma: 's', 'r', 'c', 'p', 'u' |
| `/opencv/target_point` | `geometry_msgs/PointStamped` | 10 Hz | PosiciÃ³n 3D (x, y, z) en metros |
| `/opencv/target_joint` | `std_msgs/Float64MultiArray` | 10 Hz | Ãngulos de joints sugeridos |
| `/opencv/detection_info` | `std_msgs/String` | 10 Hz | InformaciÃ³n completa en JSON |

#### Por pincher_controller:

| TÃ³pico | Tipo | Frecuencia | DescripciÃ³n |
|:-------|:-----|:-----------|:------------|
| `/joint_states` | `sensor_msgs/JointState` | 10 Hz | Estado actual de todos los joints |
| `/parameter_events` | `rcl_interfaces/ParameterEvent` | Event-based | Cambios de parÃ¡metros |

### Formato del JSON en /opencv/detection_info

```json
{
  "shape_name": "circle",
  "shape_code": "c",
  "confidence": 0.92,
  "r_cm": 5.2,
  "theta_deg": 45.0,
  "x_m": 0.15,
  "y_m": 0.08,
  "z_m": -0.025,
  "timestamp": 1234567890.123
}
```

### VisualizaciÃ³n del Grafo

```bash
# Ver grafo de nodos en tiempo real
rqt_graph

# Listar todos los nodos activos
ros2 node list

# Ver informaciÃ³n de un nodo especÃ­fico
ros2 node info /opencv_detector_node
ros2 node info /pincher_controller

# Listar todos los tÃ³picos
ros2 topic list

# Ver frecuencia de publicaciÃ³n
ros2 topic hz /opencv/image
ros2 topic hz /joint_states

# Inspeccionar mensaje
ros2 topic echo /opencv/detection_info

# Ver estructura de mensaje
ros2 interface show sensor_msgs/msg/Image
ros2 interface show geometry_msgs/msg/PointStamped
```

---

## ğŸ“· ConfiguraciÃ³n de CÃ¡mara

### Hardware Utilizado

<div align="center">
  <img src="./sources/camara.png" alt="CÃ¡mara Logitech C270" width="400"/>
  <p><em>CÃ¡mara web Logitech HD C270</em></p>
</div>

### Especificaciones

| ParÃ¡metro | Valor |
|:----------|:------|
| **Modelo** | Logitech HD C270 |
| **ResoluciÃ³n MÃ¡xima** | 1280Ã—720 (720p) |
| **ResoluciÃ³n de Trabajo** | 640Ã—480 |
| **FPS** | 30 |
| **Campo Visual** | 55Â° diagonal |
| **Enfoque** | Fijo |
| **MicrÃ³fono** | Integrado mono (1.5m alcance) |
| **ConexiÃ³n** | USB 2.0 tipo A |
| **Montaje** | Clip universal + trÃ­pode |

### ConfiguraciÃ³n en CÃ³digo

```python
# ParÃ¡metros de captura
cap = cv2.VideoCapture(0)  # Ãndice de cÃ¡mara

# Configurar resoluciÃ³n
cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)

# FPS
cap.set(cv2.CAP_PROP_FPS, 30)

# Codec (MJPG para mejor performance)
cap.set(cv2.CAP_PROP_FOURCC, cv2.VideoWriter_fourcc(*"MJPG"))

# Desactivar autofocus (si disponible)
cap.set(cv2.CAP_PROP_AUTOFOCUS, 0)
cap.set(cv2.CAP_PROP_FOCUS, 50)  # Valor fijo

# ExposiciÃ³n manual para consistencia
cap.set(cv2.CAP_PROP_AUTO_EXPOSURE, 0.25)  # Manual mode
cap.set(cv2.CAP_PROP_EXPOSURE, -6)         # Valor especÃ­fico
```

### Verificar Dispositivos de CÃ¡mara

```bash
# Listar dispositivos de video disponibles
ls -l /dev/video*

# Ver informaciÃ³n detallada de cÃ¡mara
v4l2-ctl --list-devices

# Ver formatos soportados
v4l2-ctl --device=/dev/video0 --list-formats-ext

# Ajustar permisos si es necesario
sudo chmod 666 /dev/video0
```

### Rangos de Color HSV

Calibrados para iluminaciÃ³n de laboratorio:

```python
# Naranja (objetos a detectar)
lower_orange = np.array([3, 70, 70], dtype=np.uint8)
upper_orange = np.array([28, 255, 255], dtype=np.uint8)

# Blanco (disco de referencia)
lower_white = np.array([0, 0, 200], dtype=np.uint8)
upper_white = np.array([180, 30, 255], dtype=np.uint8)
```

Para recalibrar segÃºn tu iluminaciÃ³n:

```python
def calibrate_hsv_range():
    """Herramienta interactiva para calibrar rangos HSV"""
    cap = cv2.VideoCapture(0)
    
    # Crear ventana con sliders
    cv2.namedWindow('HSV Calibration')
    cv2.createTrackbar('H_min', 'HSV Calibration', 0, 179, lambda x: None)
    cv2.createTrackbar('H_max', 'HSV Calibration', 179, 179, lambda x: None)
    cv2.createTrackbar('S_min', 'HSV Calibration', 0, 255, lambda x: None)
    cv2.createTrackbar('S_max', 'HSV Calibration', 255, 255, lambda x: None)
    cv2.createTrackbar('V_min', 'HSV Calibration', 0, 255, lambda x: None)
    cv2.createTrackbar('V_max', 'HSV Calibration', 255, 255, lambda x: None)
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
        
        # Leer valores de sliders
        h_min = cv2.getTrackbarPos('H_min', 'HSV Calibration')
        h_max = cv2.getTrackbarPos('H_max', 'HSV Calibration')
        s_min = cv2.getTrackbarPos('S_min', 'HSV Calibration')
        s_max = cv2.getTrackbarPos('S_max', 'HSV Calibration')
        v_min = cv2.getTrackbarPos('V_min', 'HSV Calibration')
        v_max = cv2.getTrackbarPos('V_max', 'HSV Calibration')
        
        # Aplicar mÃ¡scara
        lower = np.array([h_min, s_min, v_min])
        upper = np.array([h_max, s_max, v_max])
        mask = cv2.inRange(hsv, lower, upper)
        
        # Mostrar resultado
        result = cv2.bitwise_and(frame, frame, mask=mask)
        cv2.imshow('HSV Calibration', np.hstack([frame, result]))
        
        if cv2.waitKey(1) & 0xFF == ord('q'):
            print(f"lower = np.array([{h_min}, {s_min}, {v_min}])")
            print(f"upper = np.array([{h_max}, {s_max}, {v_max}])")
            break
    
    cap.release()
    cv2.destroyAllWindows()
```

---

## âš™ï¸ CalibraciÃ³n del Sistema

### Offset CÃ¡mara-Robot

Los offsets compensan la diferencia entre el frame de la cÃ¡mara y el frame del robot:

**En opencv_detector.py:**
```python
# Offset de calibraciÃ³n (ajustar segÃºn tu setup)
OPENCV_OFFSET_X_M = -0.1   # Offset en X (metros)
OPENCV_OFFSET_Y_M = 0.0    # Offset en Y (metros)
```

**En control_servo2.py:**
```python
# IMPORTANTE: Deben coincidir con los valores del detector
OPENCV_OFFSET_X_M = -0.1
OPENCV_OFFSET_Y_M = 0.0
```

### Procedimiento de CalibraciÃ³n

1. **Medir Radio del Disco**
   ```python
   # Medir diÃ¡metro fÃ­sico del disco blanco
   diameter_cm = 14.5  # Ejemplo
   DISK_RADIUS_CM = diameter_cm / 2  # = 7.25 cm
   ```

2. **Calibrar Offsets X e Y**
   
   a. Colocar un objeto de prueba en posiciÃ³n conocida
   
   b. Ejecutar detector y anotar coordenadas detectadas:
      ```bash
      ros2 topic echo /opencv/detection_info
      # Anotar: x_detected, y_detected
      ```
   
   c. Mover robot manualmente a esa posiciÃ³n:
      ```bash
      # En GUI: ir a pestaÃ±a Control XYZ
      # Mover a posiciÃ³n del objeto
      # Anotar: x_real, y_real
      ```
   
   d. Calcular offsets:
      ```python
      OPENCV_OFFSET_X_M = x_real - x_detected
      OPENCV_OFFSET_Y_M = y_real - y_detected
      ```
   
   e. Actualizar en ambos archivos:
      - `opencv_detector.py`
      - `control_servo2.py`
   
   f. Recompilar:
      ```bash
      cd ~/ros2_ws/phantom_ws
      colcon build --packages-select pincher_control
      source install/setup.bash
      ```

3. **Calibrar Altura (Z)**
   ```python
   # Medir altura de la mesa respecto al origen del robot
   # Ajustar Z_OFFSET
   Z_OFFSET_M = -0.025  # Ejemplo: 2.5cm bajo origen
   ```

4. **Verificar CalibraciÃ³n**
   
   a. Colocar objeto en 3-4 posiciones diferentes
   
   b. Para cada posiciÃ³n:
      - Ejecutar detector
      - Ejecutar rutina pick & place
      - Verificar precisiÃ³n
   
   c. Si hay errores sistemÃ¡ticos, ajustar offsets iterativamente

### Configurar Puntos de Destino (Drop Zones)

**En control_servo2.py:**

```python
# Puntos de drop por forma
DROP_POINTS = {
    's': (0.15, -0.15, -0.02),   # Cuadrado â†’ Rojo
    'r': (0.15, 0.15, -0.02),    # RectÃ¡ngulo â†’ Verde
    'c': (0.20, 0.0, -0.02),     # CÃ­rculo â†’ Azul
    'p': (0.12, 0.0, -0.02),     # PentÃ¡gono â†’ Amarillo
}
```

Para calibrar cada punto:

1. Colocar contenedor en posiciÃ³n deseada
2. Usar GUI pestaÃ±a "Control XYZ" para mover robot sobre contenedor
3. Anotar coordenadas (x, y, z)
4. Actualizar `DROP_POINTS`
5. Verificar que estÃ© dentro del workspace:
   ```python
   def is_position_safe(self, xyz):
       x, y, z = xyz
       r = np.sqrt(x**2 + y**2)
       return (0.05 < r < 0.30) and (z > -0.05)
   ```

---

## ğŸš€ EjecuciÃ³n del Sistema

### OpciÃ³n 1: Launch File Unificado (Recomendado)

Lanza ambos nodos simultÃ¡neamente:

```bash
# Terminal Ãºnica
ros2 launch pincher_control pincher_opencv.launch.py

# Con parÃ¡metros personalizados
ros2 launch pincher_control pincher_opencv.launch.py \
    camera_index:=1 \
    port:=/dev/ttyUSB1 \
    publish_rate:=30.0
```

### OpciÃ³n 2: Nodos Separados

Permite mayor flexibilidad y debugging:

```bash
# Terminal 1: Detector OpenCV
ros2 run pincher_control opencv_detector

# Terminal 2: Controlador del Robot
ros2 run pincher_control control_servo

# Terminal 3 (opcional): Monitor de tÃ³picos
ros2 topic echo /opencv/detection_info
```

### OpciÃ³n 3: Con ParÃ¡metros Personalizados

```bash
# Detector con cÃ¡mara externa y mayor frecuencia
ros2 run pincher_control opencv_detector \
    --ros-args \
    -p camera_index:=1 \
    -p publish_rate:=30.0

# Controlador con puerto USB alternativo
ros2 run pincher_control control_servo \
    --ros-args \
    -p port:='/dev/ttyUSB1' \
    -p baudrate:=57600
```

### Flujo de Trabajo TÃ­pico

1. **Iniciar Sistema**
   ```bash
   ros2 launch pincher_control pincher_opencv.launch.py
   ```

2. **Verificar ComunicaciÃ³n**
   ```bash
   # Nueva terminal
   ros2 topic list | grep opencv
   # Debe mostrar:
   # /opencv/image
   # /opencv/shape
   # /opencv/target_point
   # /opencv/detection_info
   
   ros2 topic hz /opencv/image
   # Debe mostrar ~10 Hz
   ```

3. **Colocar Objeto en Disco Blanco**
   - Figura naranja sobre disco blanco
   - Centrada lo mejor posible

4. **En la GUI:**
   - Navegar a pestaÃ±a "ğŸ“· OpenCV"
   - Verificar que aparezca video con overlays
   - Esperar a detecciÃ³n (indicador verde)
   - Click en "ğŸ“Œ Guardar dato actual"
   - Verificar informaciÃ³n en panel inferior
   - Click en "ğŸ¤– Ejecutar rutina"
   - Confirmar ejecuciÃ³n
   - Observar secuencia de pick & place

5. **Repetir con Diferentes Formas**
   - Probar con cÃ­rculo, cuadrado, rectÃ¡ngulo, pentÃ¡gono
   - Verificar que cada forma va a su contenedor correcto

### VisualizaciÃ³n Adicional

```bash
# Ver imagen procesada en ventana separada
ros2 run rqt_image_view rqt_image_view /opencv/image

# VisualizaciÃ³n 3D del robot en RViz
rviz2 -d ~/ros2_ws/phantom_ws/install/phantomx_pincher_description/share/phantomx_pincher_description/config/pincher_rviz.rviz

# Monitor de tÃ³picos con rqt
rqt
```

### Grabar SesiÃ³n

```bash
# Grabar todos los tÃ³picos de OpenCV
ros2 bag record /opencv/image /opencv/shape /opencv/target_point /opencv/detection_info /joint_states

# Grabar todo
ros2 bag record -a

# Reproducir grabaciÃ³n
ros2 bag play <bag_file.db3>
```

---

## ğŸ› Troubleshooting

### âŒ "Esperando imagen de /opencv/image..."

**Problema:** La GUI no recibe imÃ¡genes del detector.

**DiagnÃ³stico:**
```bash
# Verificar que opencv_detector estÃ© corriendo
ros2 node list | grep opencv

# Ver si el tÃ³pico existe
ros2 topic list | grep opencv/image

# Verificar frecuencia de publicaciÃ³n
ros2 topic hz /opencv/image
```

**Soluciones:**

1. **Detector no estÃ¡ corriendo:**
   ```bash
   ros2 run pincher_control opencv_detector
   ```

2. **CÃ¡mara no disponible:**
   ```bash
   # Verificar dispositivos
   ls -l /dev/video*
   
   # Ajustar permisos
   sudo chmod 666 /dev/video0
   
   # Probar Ã­ndice diferente
   ros2 run pincher_control opencv_detector --ros-args -p camera_index:=1
   ```

3. **Problema de DDS/Discovery:**
   ```bash
   # Configurar mismo ROS_DOMAIN_ID
   export ROS_DOMAIN_ID=42
   
   # En ambas terminales
   source ~/ros2_ws/phantom_ws/install/setup.bash
   ```

### âŒ "No hay detecciÃ³n disponible"

**Problema:** No se detectan figuras.

**DiagnÃ³stico:**
```bash
# Ver quÃ© estÃ¡ detectando
ros2 topic echo /opencv/detection_info

# Ver imagen procesada
ros2 run rqt_image_view rqt_image_view /opencv/image
```

**Soluciones:**

1. **Verificar colores:**
   - Figuras deben ser **naranja**
   - Disco debe ser **blanco**
   - IluminaciÃ³n adecuada (no muy brillante ni muy oscura)

2. **Ajustar rangos HSV:**
   Editar en `opencv_detector.py`:
   ```python
   # Si objetos son mÃ¡s rojos
   lower_orange = np.array([0, 70, 70])
   
   # Si objetos son mÃ¡s amarillos
   upper_orange = np.array([35, 255, 255])
   ```

3. **TamaÃ±o de objetos:**
   Verificar filtros de Ã¡rea en cÃ³digo:
   ```python
   # Disco debe ser > 5000 pÃ­xelesÂ²
   # Figuras deben ser > 500 pÃ­xelesÂ²
   ```

4. **PosiciÃ³n:**
   - Objetos dentro del Ã¡rea del disco
   - No demasiado cerca del borde

### âŒ El robot se mueve a posiciÃ³n incorrecta

**Problema:** DesalineaciÃ³n entre cÃ¡mara y robot.

**DiagnÃ³stico:**
```bash
# Ver coordenadas detectadas
ros2 topic echo /opencv/target_point

# Comparar con posiciÃ³n real del objeto
```

**Soluciones:**

1. **Recalibrar offsets:**
   ```python
   # En ambos archivos
   OPENCV_OFFSET_X_M = <valor_calibrado>
   OPENCV_OFFSET_Y_M = <valor_calibrado>
   ```
   Ver [CalibraciÃ³n del Sistema](#-calibraciÃ³n-del-sistema)

2. **Verificar radio del disco:**
   ```python
   DISK_RADIUS_CM = 7.25  # Medir y ajustar
   ```

3. **Transformaciones inconsistentes:**
   Verificar que los offsets sean iguales en:
   - `opencv_detector.py`
   - `control_servo2.py`

### âŒ "Camera not available"

**Problema:** opencv_detector no puede abrir la cÃ¡mara.

**Soluciones:**

1. **Probar Ã­ndices diferentes:**
   ```bash
   # Probar 0, 1, 2...
   ros2 run pincher_control opencv_detector --ros-args -p camera_index:=1
   ```

2. **Verificar que no estÃ© en uso:**
   ```bash
   # Cerrar otras aplicaciones usando cÃ¡mara
   # (Cheese, browser, etc.)
   
   # Ver procesos usando video
   lsof /dev/video0
   ```

3. **Permisos:**
   ```bash
   sudo usermod -aG video $USER
   # Logout y login nuevamente
   ```

### âŒ cv_bridge no encontrado

**Problema:** Error importando CvBridge.

**SoluciÃ³n:**
```bash
sudo apt install ros-jazzy-cv-bridge

# Si instalado pero no encuentra:
source /opt/ros/jazzy/setup.bash
source ~/ros2_ws/phantom_ws/install/setup.bash
```

### âŒ Dynamixel communication error

**Problema:** No puede comunicar con motores.

**DiagnÃ³stico:**
```bash
# Verificar puerto
ls -l /dev/ttyUSB*

# Ver permisos
groups $USER | grep dialout
```

**Soluciones:**

1. **Puerto incorrecto:**
   ```bash
   ros2 run pincher_control control_servo --ros-args -p port:='/dev/ttyUSB1'
   ```

2. **Sin permisos:**
   ```bash
   sudo usermod -aG dialout $USER
   # Logout y login
   
   # O temporalmente:
   sudo chmod 666 /dev/ttyUSB0
   ```

3. **Baudrate incorrecto:**
   ```bash
   ros2 run pincher_control control_servo --ros-args -p baudrate:=57600
   ```

4. **Verificar conexiones:**
   - Cable USB conectado
   - AlimentaciÃ³n externa conectada (12V)
   - LED de USB2Dynamixel encendido

### âŒ IK no converge

**Problema:** "IK no convergiÃ³" en logs.

**Causas:**
- PosiciÃ³n fuera del workspace
- Cerca de singularidad
- Offsets mal calibrados

**Soluciones:**

1. **Verificar workspace:**
   ```python
   r = sqrt(xÂ² + yÂ²)
   # Debe estar entre 5cm y 30cm
   
   z > -0.05  # No mÃ¡s bajo que -5cm
   ```

2. **Evitar singularidades:**
   - No colocar objetos directamente bajo el robot (xâ‰ˆ0, yâ‰ˆ0)
   - Mantener distancia radial > 5cm

3. **Aumentar iteraciones:**
   ```python
   # En control_servo2.py
   inverse_kinematics_lm(target, max_iterations=200)
   ```

### âŒ Gripper no agarra correctamente

**Problema:** Objeto se cae o gripper no cierra.

**Soluciones:**

1. **Ajustar fuerza de agarre:**
   ```python
   GRIPPER_CLOSED = 700  # Aumentar valor (cuidado con objetos frÃ¡giles)
   ```

2. **Aumentar tiempo de espera:**
   ```python
   self.set_gripper_position(GRIPPER_CLOSED)
   time.sleep(1.5)  # Dar mÃ¡s tiempo para cerrar
   ```

3. **Verificar alineaciÃ³n:**
   - Gripper debe estar perpendicular al objeto
   - Ajustar altura de pick si es necesario

### âŒ Logs de ayuda

```bash
# Ver logs del nodo detector
ros2 run pincher_control opencv_detector --ros-args --log-level debug

# Ver logs del controlador
ros2 run pincher_control control_servo --ros-args --log-level info

# Logs de todo el sistema
ros2 launch pincher_control pincher_opencv.launch.py --ros-args --log-level debug
```

---

## ğŸ“ CinemÃ¡tica del Robot

### ParÃ¡metros Denavit-Hartenberg

<div align="center">
  <img src="./sources/DH.png" alt="Diagrama DH" width="600"/>
  <p><em>Diagrama Denavit-Hartenberg del PhantomX Pincher X100</em></p>
</div>

| Joint | Î¸ (rad) | d (m) | a (m) | Î± (rad) | LÃ­mites (Â°) |
|:------|:--------|:------|:------|:--------|:------------|
| **1** | qâ‚ | 0.067 | 0 | Ï€/2 | [-150, 150] |
| **2** | qâ‚‚ - Ï€/2 | 0 | 0.105 | 0 | [-110, 110] |
| **3** | qâ‚ƒ | 0 | 0.105 | 0 | [-110, 110] |
| **4** | qâ‚„ | 0 | 0.110 | 0 | [-110, 110] |

### CinemÃ¡tica Directa

TransformaciÃ³n del frame base al end-effector:

```python
def forward_kinematics(q):
    """
    Calcula pose del end-effector dada configuraciÃ³n de joints.
    
    Args:
        q: [q1, q2, q3, q4] en radianes
    
    Returns:
        T: Matriz 4Ã—4 de transformaciÃ³n homogÃ©nea
    """
    # ParÃ¡metros DH
    d1 = 0.067  # Altura base
    a2 = 0.105  # Longitud link 2
    a3 = 0.105  # Longitud link 3
    a4 = 0.110  # Longitud end-effector
    
    q1, q2, q3, q4 = q
    
    # Matrices de transformaciÃ³n individuales
    T01 = dh_matrix(q1, d1, 0, np.pi/2)
    T12 = dh_matrix(q2 - np.pi/2, 0, a2, 0)
    T23 = dh_matrix(q3, 0, a3, 0)
    T34 = dh_matrix(q4, 0, a4, 0)
    
    # ComposiciÃ³n
    T = T01 @ T12 @ T23 @ T34
    
    return T

def dh_matrix(theta, d, a, alpha):
    """Matriz de transformaciÃ³n DH individual"""
    return np.array([
        [np.cos(theta), -np.sin(theta)*np.cos(alpha),  np.sin(theta)*np.sin(alpha), a*np.cos(theta)],
        [np.sin(theta),  np.cos(theta)*np.cos(alpha), -np.cos(theta)*np.sin(alpha), a*np.sin(theta)],
        [0,              np.sin(alpha),                 np.cos(alpha),                d              ],
        [0,              0,                             0,                            1              ]
    ])
```

### Workspace del Robot

Espacio alcanzable del end-effector:

```python
def compute_workspace():
    """
    Calcula y visualiza el workspace del robot.
    """
    import matplotlib.pyplot as plt
    from mpl_toolkits.mplot3d import Axes3D
    
    points = []
    
    # Samplear configuraciones
    for q1 in np.linspace(-150, 150, 20) * np.pi/180:
        for q2 in np.linspace(-110, 110, 20) * np.pi/180:
            for q3 in np.linspace(-110, 110, 20) * np.pi/180:
                for q4 in np.linspace(-110, 110, 10) * np.pi/180:
                    q = [q1, q2, q3, q4]
                    T = forward_kinematics(q)
                    pos = T[0:3, 3]
                    points.append(pos)
    
    points = np.array(points)
    
    # Plot
    fig = plt.figure(figsize=(10, 8))
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(points[:,0], points[:,1], points[:,2], 
               c=points[:,2], cmap='viridis', s=1, alpha=0.3)
    ax.set_xlabel('X (m)')
    ax.set_ylabel('Y (m)')
    ax.set_zlabel('Z (m)')
    ax.set_title('Workspace del PhantomX Pincher X100')
    plt.show()
```

**CaracterÃ­sticas del Workspace:**
- Radio alcanzable: ~5 cm a ~30 cm
- Altura Ãºtil: -5 cm a +25 cm (respecto a base)
- Forma aproximada: toroide
- Singularidad en eje Z (x=0, y=0)

### AnÃ¡lisis de Jacobiano

```python
def analyze_manipulability(q):
    """
    Calcula Ã­ndice de manipulabilidad de Yoshikawa.
    
    Args:
        q: configuraciÃ³n de joints
    
    Returns:
        w: Ã­ndice de manipulabilidad (0-1)
    """
    J = geometric_jacobian(q)
    J_pos = J[0:3, :]  # Solo parte posicional
    
    # Manipulabilidad de Yoshikawa
    w = np.sqrt(np.linalg.det(J_pos @ J_pos.T))
    
    return w

def find_singularities():
    """
    Encuentra configuraciones singulares (det(J) â‰ˆ 0).
    """
    singularities = []
    
    for q1 in np.linspace(-150, 150, 50) * np.pi/180:
        for q2 in np.linspace(-110, 110, 50) * np.pi/180:
            for q3 in np.linspace(-110, 110, 50) * np.pi/180:
                q = [q1, q2, q3, 0]
                w = analyze_manipulability(q)
                
                if w < 0.001:  # Cerca de singularidad
                    T = forward_kinematics(q)
                    pos = T[0:3, 3]
                    singularities.append(pos)
    
    return np.array(singularities)
```

---

## ğŸ“ Plano de Planta

### PosiciÃ³n Inicial

<div align="center">
  <img src="./sources/plano_inicial.jpg" alt="Plano Inicial" width="600"/>
  <p><em>DisposiciÃ³n inicial con pieza en el centro del disco blanco</em></p>
</div>

**Setup:**
- Disco blanco de referencia (diÃ¡metro: 14.5 cm)
- Figura naranja centrada en el disco
- CÃ¡mara cenital sobre Ã¡rea de trabajo
- Robot en posiciÃ³n home

### PosiciÃ³n Final

<div align="center">
  <img src="./sources/plano_final.jpg" alt="Plano Final" width="600"/>
  <p><em>DisposiciÃ³n de contenedores codificados por color</em></p>
</div>

**Contenedores:**
| Color | Forma | UbicaciÃ³n (x, y, z) |
|:------|:------|:-------------------|
| ğŸ”´ Rojo | Cuadrado | (0.15, -0.15, -0.02) |
| ğŸŸ¢ Verde | RectÃ¡ngulo | (0.15, 0.15, -0.02) |
| ğŸ”µ Azul | CÃ­rculo | (0.20, 0.0, -0.02) |
| ğŸŸ¡ Amarillo | PentÃ¡gono | (0.12, 0.0, -0.02) |

---

## ğŸ“ Conclusiones

### Logros TÃ©cnicos

1. **Arquitectura Distribuida Exitosa**
   - ImplementaciÃ³n de nodos ROS 2 independientes permitiÃ³ modularidad y escalabilidad
   - ComunicaciÃ³n robusta mediante tÃ³picos estandarizados
   - FÃ¡cil debugging y mantenimiento de componentes

2. **VisiÃ³n Artificial Robusta**
   - Sistema de scoring multicriterio logrÃ³ clasificaciones precisas (>90% confianza)
   - DetecciÃ³n invariante a rotaciÃ³n y cambios de iluminaciÃ³n moderados
   - CÃ¡lculo preciso de coordenadas polares y transformaciÃ³n 3D

3. **Control Preciso**
   - CinemÃ¡tica inversa con Levenberg-Marquardt demostrÃ³ convergencia confiable
   - Estrategia multi-seed aumentÃ³ robustez ante posiciones desafiantes
   - ValidaciÃ³n de workspace previno movimientos peligrosos

4. **Interfaz Profesional**
   - GUI PyQt5 abstrae complejidad tÃ©cnica del sistema
   - OperaciÃ³n intuitiva sin conocimiento profundo de ROS 2
   - VisualizaciÃ³n en tiempo real facilita debugging

### Lecciones Aprendidas

1. **CalibraciÃ³n es CrÃ­tica**
   - PrecisiÃ³n del sistema depende fuertemente de calibraciÃ³n cÃ¡mara-robot
   - Proceso iterativo de ajuste de offsets es necesario
   - Mediciones fÃ­sicas precisas del disco son fundamentales

2. **IluminaciÃ³n Importa**
   - Rangos HSV deben ajustarse segÃºn condiciones de iluminaciÃ³n
   - Luz difusa y consistente mejora detecciones
   - Evitar sombras y reflejos en disco blanco

3. **Robustez sobre Velocidad**
   - Scoring multicriterio mÃ¡s lento pero mÃ¡s preciso que mÃ©todos simples
   - Validaciones de seguridad previenen daÃ±os al robot
   - Trade-off entre performance y confiabilidad se decanta por confiabilidad

4. **Modularidad Facilita Desarrollo**
   - SeparaciÃ³n en nodos permitiÃ³ trabajo paralelo del equipo
   - Pruebas independientes de visiÃ³n y control aceleraron debugging
   - ReutilizaciÃ³n de detector en otros proyectos es viable

### Mejoras Futuras

1. **VisiÃ³n**
   - Implementar detecciÃ³n de mÃºltiples objetos simultÃ¡neos
   - Agregar clasificaciÃ³n por color ademÃ¡s de forma
   - Mejorar detecciÃ³n con deep learning (YOLO, etc.)

2. **Control**
   - PlanificaciÃ³n de trayectorias con MoveIt2 para movimientos mÃ¡s suaves
   - EvitaciÃ³n de obstÃ¡culos dinÃ¡mica
   - Control de fuerza para objetos frÃ¡giles

3. **Sistema**
   - Integrar mÃ¡s sensores (fuerza, proximidad)
   - Implementar sistema de recuperaciÃ³n ante fallos
   - Agregar logging y analytics para optimizaciÃ³n

4. **Interfaz**
   - Agregar modo de entrenamiento para nuevas formas
   - Dashboard web para monitoreo remoto
   - IntegraciÃ³n con bases de datos para tracking de objetos

### Impacto Educativo

Este proyecto demuestra la integraciÃ³n efectiva de:
- TeorÃ­a de robÃ³tica (cinemÃ¡tica, control)
- VisiÃ³n artificial aplicada
- IngenierÃ­a de software (arquitecturas distribuidas)
- PrÃ¡cticas de desarrollo modernas (ROS 2, Git)

El sistema resultante sirve como plataforma educativa para futuros estudiantes y como base para proyectos mÃ¡s avanzados de manipulaciÃ³n robÃ³tica.

---

## ğŸ“š Referencias

1. **Laboratorio de RobÃ³tica - Universidad Nacional de Colombia.** GuÃ­as de laboratorio del curso de RobÃ³tica, 2025.

2. ROBOTIS. *DYNAMIXEL SDK Manual.* DocumentaciÃ³n oficial para comunicaciÃ³n con servomotores Dynamixel.  
   https://emanual.robotis.com/docs/en/software/dynamixel/dynamixel_sdk/

3. Open Robotics. *ROS 2 Jazzy Documentation.*  
   https://docs.ros.org/en/jazzy/

4. Open Robotics. *ROS 2 Launch File Documentation.*  
   https://docs.ros.org/en/jazzy/Tutorials/Intermediate/Launch/Launch-Main.html

5. MoveIt. *MoveIt 2 Documentation.*  
   https://moveit.ros.org/

6. Corke, P. *Robotics Toolbox for Python.* GitHub Repository.  
   https://github.com/petercorke/robotics-toolbox-python

7. OpenCV. *OpenCV-Python Tutorials.*  
   https://docs.opencv.org/master/d6/d00/tutorial_py_root.html

8. OpenCV. *Contour Features - Shape Descriptors.*  
   https://docs.opencv.org/4.x/dd/d49/tutorial_py_contour_features.html

9. Qt Company. *PyQt5 Documentation.*  
   https://www.riverbankcomputing.com/static/Docs/PyQt5/

10. Craig, J.J. *Introduction to Robotics: Mechanics and Control.* Pearson, 3rd Edition, 2005.

11. Spong, M.W., Hutchinson, S., Vidyasagar, M. *Robot Modeling and Control.* Wiley, 2nd Edition, 2020.

12. Corke, P. *Robotics, Vision and Control: Fundamental Algorithms in MATLAB.* Springer, 2nd Edition, 2017.

13. Trossen Robotics. *PhantomX Pincher Robot Arm Assembly Guide.*  
    https://www.trossenrobotics.com/

14. Levenberg, K. *A Method for the Solution of Certain Non-Linear Problems in Least Squares.* Quarterly of Applied Mathematics, 1944.

15. Marquardt, D.W. *An Algorithm for Least-Squares Estimation of Nonlinear Parameters.* Journal of the Society for Industrial and Applied Mathematics, 1963.

---

## ğŸ“„ Licencia

Este proyecto estÃ¡ bajo la Licencia MIT. Ver el archivo `LICENSE` para mÃ¡s detalles.

```
MIT License

Copyright (c) 2025 Universidad Nacional de Colombia

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
```

---

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Para contribuir:

1. Fork el repositorio
2. Crea una rama para tu feature:
   ```bash
   git checkout -b feature/AmazingFeature
   ```
3. Commit tus cambios:
   ```bash
   git commit -m 'Add: AmazingFeature'
   ```
4. Push a la rama:
   ```bash
   git push origin feature/AmazingFeature
   ```
5. Abre un Pull Request

### EstÃ¡ndares de CÃ³digo

- **Python:** PEP 8
- **Commits:** Conventional Commits
- **DocumentaciÃ³n:** Google Style docstrings
- **Testing:** pytest para pruebas unitarias

---

## ğŸ“§ Contacto

**Samuel David SÃ¡nchez CÃ¡rdenas** (Lead Developer)  
ğŸ“§ Email: samsanchezca@unal.edu.co  
ğŸ”— GitHub: [@samsanchezcar](https://github.com/samsanchezcar)

**Equipo de Desarrollo:**
- **Santiago Ãvila Corredor** - savilaco@unal.edu.co - [@Santiago-Avila](https://github.com/Santiago-Avila)
- **Santiago MariÃ±o CortÃ©s** - smarinoc@unal.edu.co - [@mrbrightside8](https://github.com/mrbrightside8)
- **Juan Ãngel Vargas RodrÃ­guez** - juvargasro@unal.edu.co - [@juvargasro](https://github.com/juvargasro)
- **Juan JosÃ© Delgado Estrada** - judelgadoe@unal.edu.co - [@Juan-delgado1](https://github.com/Juan-delgado1)

---

## ğŸ“– Referencia RÃ¡pida de Comandos

```bash
# ============================================================
#  INSTALACIÃ“N
# ============================================================

# Clonar repositorio
git clone https://github.com/samsanchezcar/Phantom_X_Pincher_ROS2_OpenCV.git

# Compilar workspace
cd ~/ros2_ws/phantom_ws
colcon build --symlink-install
source install/setup.bash

# ============================================================
#  EJECUCIÃ“N
# ============================================================

# Launch unificado (recomendado)
ros2 launch pincher_control pincher_opencv.launch.py

# Nodos separados
ros2 run pincher_control opencv_detector
ros2 run pincher_control control_servo

# Con parÃ¡metros
ros2 run pincher_control opencv_detector --ros-args -p camera_index:=1
ros2 run pincher_control control_servo --ros-args -p port:='/dev/ttyUSB1'

# ============================================================
#  DEBUGGING
# ============================================================

# Listar nodos
ros2 node list

# Listar tÃ³picos
ros2 topic list

# Ver frecuencia
ros2 topic hz /opencv/image

# Ver mensajes
ros2 topic echo /opencv/detection_info

# Visualizar imagen
ros2 run rqt_image_view rqt_image_view /opencv/image

# Grafo de nodos
rqt_graph

# ============================================================
#  VISUALIZACIÃ“N
# ============================================================

# RViz
rviz2 -d <path_to_rviz_config>

# Robotics Toolbox
ros2 run pincher_control toolbox

# ============================================================
#  GRABACIÃ“N
# ============================================================

# Grabar tÃ³picos OpenCV
ros2 bag record /opencv/image /opencv/shape /opencv/target_point /opencv/detection_info

# Grabar todo
ros2 bag record -a

# Reproducir
ros2 bag play <bag_file.db3>

# ============================================================
#  VERIFICACIÃ“N
# ============================================================

# Verificar ROS 2
echo $ROS_DISTRO  # Debe mostrar: jazzy

# Verificar cÃ¡maras
ls -l /dev/video*
v4l2-ctl --list-devices

# Verificar puerto USB
ls -l /dev/ttyUSB*

# Permisos
sudo usermod -aG dialout $USER
sudo usermod -aG video $USER
```

---

<div align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=gradient&height=100&section=footer&text=Universidad%20Nacional%20de%20Colombia%20%E2%80%A2%202025&fontSize=18&animation=fadeIn" width="100%" />
  
  **ğŸ¤– Desarrollado con pasiÃ³n por la robÃ³tica ğŸ¤–**
  
  â­ Si este proyecto te fue Ãºtil, considera darle una estrella en GitHub â­
</div>
